{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUbP4HqQ8JP4"
      },
      "source": [
        "### Part (a)\n",
        "#### Load and assess \"cx_churn_data.csv\" dataset.\n",
        "\n",
        "Explain how data was loaded..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVcroMhr8JP4"
      },
      "outputs": [],
      "source": [
        "# Insert code for question (a)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data set import\n",
        "data = pd.read_csv('cx_churn_data.csv', encoding='latin-1')"
      ],
      "metadata": {
        "id": "Cwa9Rsmq-oHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "metadata": {
        "id": "Xdrq4I_--q1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "rd-2deob-u4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2GTq_T_8JP6"
      },
      "source": [
        "Explain the challenges encountered and describe dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Storage specific datasets**\n",
        "To meet special storage needs, teams will occasionally construct datasets that are storage-specific. Teams using them will inevitably copy and modify them, creating latency and multiple copies. Datasets ought to be independent of storage.\n",
        "Dataset analytics\n",
        "It is essential to understand the trends in usage and access of various datasets. It gives the data science team a means to determine which datasets are more popular, which ones need to be maintained more frequently, and which datasets should be deleted. Data quality management is expensive and necessary. The team manages data quality for the appropriate datasets with the aid of dataset analytics.\n",
        "\n",
        "**Accessibility**\n",
        "In situations where there are too many datasets, it may be difficult for a new team member to grasp which dataset can be used to solve which problem. Teams could have trouble locating the right dataset for the issue at hand if there is no better way to access and learn about the metadata for a dataset.\n",
        "Security and Audit\n",
        "Access to datasets ought to be controlled in the same manner as access to the apps where the data originated. An audit trail for dataset access should be put up to know who has access to what. When it comes to datasets, including PII data, this becomes crucial.\n",
        "\n",
        "**Data access coupling**\n",
        "Data access issues arise specifically when it is linked to computation and data store versions. When compute processes are modified or data store versions are altered, the coupling causes problems.\n",
        "\n",
        "**Lack of standards**\n",
        "Particularly when it is connected to the computation and data store versions, data access problems occur. The linkage results in issues when data storage versions or compute processes are changed.\n",
        "Storage specific datasets\n",
        "To meet special storage needs, teams will occasionally construct datasets that are storage-specific. Teams using them will inevitably copy and modify them, creating latency and multiple copies. Datasets ought to be independent of storage.\n",
        "**Dataset analytics**\n",
        "It is essential to understand the trends in usage and access of various datasets. It gives the data science team a means to determine which datasets are more popular, which ones need to be maintained more frequently, and which datasets should be deleted. Data quality management is expensive and necessary. The team manages data quality for the appropriate datasets with the aid of dataset analytics.\n",
        "\n",
        "**Accessibility**\n",
        "In situations where there are too many datasets, it may be difficult for a new team member to grasp which dataset can be used to solve which problem. Teams could have trouble locating the right dataset for the issue at hand if there is no better way to access and learn about the metadata for a dataset.\n",
        "Security and Audit\n",
        "Access to datasets ought to be controlled in the same manner as access to the apps where the data originated. An audit trail for dataset access should be put up to know who has access to what. When it comes to datasets, including PII data, this becomes crucial.\n",
        "\n",
        "**Data access coupling**\n",
        "Data access issues arise specifically when it is linked to computation and data store versions. When compute processes are modified or data store versions are altered, the coupling causes problems.\n",
        "\n",
        "**Lack of standards**\n",
        "Particularly when it is connected to the computation and data store versions, data access problems occur. The linkage results in issues when data storage versions or compute processes are changed.\n",
        "\n",
        "\n",
        "An assortment of data is referred to as a dataset. Usually, a tabular format is used to exhibit this set. One unique variable is described in each column. As stated by the supplied question, each row refers to a specific component of the data collection. Data management encompasses this.\n",
        "\n"
      ],
      "metadata": {
        "id": "PO3qf5_o8XB_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZIqJ5t-8JP6"
      },
      "source": [
        "### Part (b)\n",
        "#### Data cleansing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAhul25s8JP6"
      },
      "outputs": [],
      "source": [
        "# Insert code for part (b)\n",
        "## senior_citizen column has values 0 and 1. Replace 0 with \"No\" and 1 with \"Yes\"\n",
        "\n",
        "\n",
        "def sCStatus(input):\n",
        "    if input == \"no\" or input == \"n\" or input == '0':\n",
        "        return 0\n",
        "    elif input == \"yes\" or input == \"y\" or input == '1':\n",
        "        return 1\n",
        "    else:\n",
        "        return input\n",
        "    \n",
        "data['senior_citizen'] = data['senior_citizen'].apply(sCStatus)\n",
        "\n",
        "data['senior_citizen'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (multiple_connections column)no phone severce values converted to \"No\"\n",
        "data['multiple_connections'] = data['multiple_connections'].replace('No phone service', 'No')\n",
        "data['multiple_connections'].value_counts()"
      ],
      "metadata": {
        "id": "bCDrdNrB_Hvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bank transfer and creadit card payment method values converted to \"Electronic check\"\n",
        "data['payment_method'] = data['payment_method'].replace(['bank transfer', 'credit card'], 'Electronic check')\n",
        "data['payment_method'].value_counts()"
      ],
      "metadata": {
        "id": "WWuE3Pss_L5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(columns=['cx_id'])"
      ],
      "metadata": {
        "id": "kwCjQrbo_O9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['partner'] = LabelEncoder().fit_transform(data['partner'])\n",
        "data['dependents'] = LabelEncoder().fit_transform(data['dependents'])\n",
        "data['phone_service'] = LabelEncoder().fit_transform(data['phone_service'])\n",
        "data['multiple_connections'] = LabelEncoder().fit_transform(data['multiple_connections'])\n",
        "data['device_insurance'] = LabelEncoder().fit_transform(data['device_insurance'])\n",
        "data['call_center_support'] = LabelEncoder().fit_transform(data['call_center_support'])\n",
        "data['dtv'] = LabelEncoder().fit_transform(data['dtv'])\n",
        "data['dtv_viu'] = LabelEncoder().fit_transform(data['dtv_viu'])\n",
        "data['e_bill'] = LabelEncoder().fit_transform(data['e_bill'])\n",
        "data['churn'] = LabelEncoder().fit_transform(data['churn'])"
      ],
      "metadata": {
        "id": "pvZbG_a3_SjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = pd.get_dummies(data)\n",
        "data.corr()['churn']"
      ],
      "metadata": {
        "id": "9MlIK6jJ_VFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting only string values from total_bill column/ converting to float\n",
        "data['total_bill'] = data['total_bill'].str.extract('(\\d+)').astype(float)"
      ],
      "metadata": {
        "id": "WbIXISYd_YAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['total_bill'].dtype"
      ],
      "metadata": {
        "id": "27nAYPaB_bbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.info()\n",
        "\n",
        "data = pd.get_dummies(data)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "c8OvHY09_eHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMdGzUcb8JP6"
      },
      "source": [
        "Describe steps taken to cleanse the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Audit\n",
        "Any data cleansing procedure begins with a careful examination of your data. You must ascertain the types and locations of the errors in your data set.\n",
        "\n",
        "How do you go about that?\n",
        "\n",
        "using statistical and database techniques that aid in the detection of discrepancies and abnormalities.\n",
        "\n",
        "There are two methods. They are\n",
        "\n",
        "Software packages\n",
        "\n",
        "Software tools let you specify a specific kind of constraint and then generate code that checks the dataset for mistakes based on the constraints' violations. Software tools can also produce reports that show which constraints have been broken, how often, and how to visualize those results.\n",
        "\n",
        "Data profiling is a technique that aids in data analysis and the production of comprehensive, educational reports about the contents of the data set. Although this approach might not be extremely thorough, it offers you a solid general understanding of the kinds of data you're working with.\n",
        "\n",
        "For instance, you can discover whether a data column complies with a specific standard, whether any values are missing, or whether one data set is related to another.\n",
        "\n",
        "\n",
        "\n",
        "2. Workflow Execution\n",
        "The procedures that make up the sequence that cleans the data sets are specified at this step. The process refers to that order.\n",
        "\n",
        "Here is an illustration of a typical data cleaning workflow, which involves performing a number of actions on the data repeatedly until the data quality is acceptable.\n",
        "\n",
        "\n",
        "\n",
        "3. Data Cleaning\n",
        "The cleaning stage entails carrying out the tasks listed in the workflow.\n",
        "\n",
        "Depending on the project's nature and the type of data, the data cleaning process may employ a variety of strategies. However, the goal at the end is always the same: data removal or repair.\n",
        "\n",
        "There are several kinds of data. They are\n",
        "\n",
        "Scrub for Duplicate\n",
        "Fix Structural Errors\n",
        "Handle Missing Data\n",
        "Check the Outliers\n",
        "Standardize or Normalize\n",
        "\n",
        "\n",
        "4. Validation\n",
        "Quality assurance is the final crucial step in the data cleansing process. Once the workflow execution is complete, you should audit the data once more to confirm that all the rules and restrictions were in fact followed.\n",
        "\n",
        "Consider the following queries to make sure your data cleansing procedure is accurate and successful:\n",
        "\n",
        "What inferences can you make about the dataset? \n",
        "Does that support or contradict your claim? \n",
        "Do you have any new information that could inform your future concepts? \n",
        "It is imperative to validate the data before giving it to a client or your boss. Inappropriate company strategy may result from false conclusions, which can be embarrassing at best and embarrassing at worst.\n",
        "\n",
        "5. Reporting\n",
        "The reporting stage is the final, but equally important, step.\n",
        "\n",
        "As far as streamlining and efficiency are concerned, creating reports and summaries of the data cleansing is crucial. Particularly if you're handling a lot of data and collaborating with lots of individuals.\n",
        "\n",
        "We can compare results and easily and quickly get insights via reports."
      ],
      "metadata": {
        "id": "WDVDbM67AU0j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJHTLvNf8JP6"
      },
      "source": [
        "### Part (c)\n",
        "#### Fill in missing values using suitable methods. (If required)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wsm2lNYg8JP7"
      },
      "outputs": [],
      "source": [
        "# Insert code for part (c)\n",
        "# check null data\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fill total_bill, monthly_bill column"
      ],
      "metadata": {
        "id": "SYQqkAmvGfiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill total_bill column\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer = imputer.fit(data[['total_bill']])\n",
        "data['total_bill'] = imputer.transform(data[['total_bill']])\n",
        "\n",
        "#fill monthly_bill column\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer = imputer.fit(data[['monthly_bill']])\n",
        "data['monthly_bill'] = imputer.transform(data[['monthly_bill']])\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "3sD8YqwX_sqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8GRcFm68JP7"
      },
      "source": [
        "Describe methods used and justify."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use the SimpleImputer"
      ],
      "metadata": {
        "id": "q3tqlpWGGZgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SimpleImputer"
      ],
      "metadata": {
        "id": "MZe5Gp55DEly"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS-1TDTb8JP7"
      },
      "source": [
        "### Part (d)\n",
        "#### Account for class imbalance with suitable methods."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Account for class imbalance with suitable methods."
      ],
      "metadata": {
        "id": "VEOPBjONGWQC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stJkCBPh8JP7"
      },
      "outputs": [],
      "source": [
        "# Insert code for part (d)\n",
        "# data.info()\n",
        "X = data.drop('churn', axis=1)\n",
        "y = data['churn']\n",
        "\n",
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "athHeXuBFpX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting balance dataset Using SMOTE"
      ],
      "metadata": {
        "id": "2W3-iz80GTGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use SMOTE to balance the data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE()\n",
        "X_smote, y_smote = smote.fit_resample(X, y)\n",
        "y_smote.value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "AoVBmcguFqQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnN4ksro8JP7"
      },
      "source": [
        "Describe methods used and justify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SDkGh6A8JP8"
      },
      "source": [
        "### Part (e)\n",
        "#### Train ML model to predict churn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train ML model to predict churn"
      ],
      "metadata": {
        "id": "3skokXQoGIS5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1G-bNYK8JP8"
      },
      "outputs": [],
      "source": [
        "# Insert code for part (e)\n",
        "# split the data into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "IGJKFmIcFxip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### try defarence methods to predict churn and compare the accuracy"
      ],
      "metadata": {
        "id": "mXW8C__zGDRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function to calculate the accuracy of the model\n",
        "def accuracy(model):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.score(X_test, y_test)\n",
        "    print(str(model) +':'+ str(y_pred))"
      ],
      "metadata": {
        "id": "znhGGPBnFzwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "accuracy(lr)\n",
        "\n",
        "## Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "accuracy(dt)\n",
        "\n",
        "## lasso\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso()\n",
        "accuracy(lasso)\n",
        "\n",
        "## Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "accuracy(rf)"
      ],
      "metadata": {
        "id": "FKgLqN_WF1-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save rf model\n",
        "import pickle\n",
        "pickle.dump(rf, open('rf_model.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "puZmLZm4JW9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save the model in a file "
      ],
      "metadata": {
        "id": "DRlKcoOgJj2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### select random forest classifier depand on accuracy"
      ],
      "metadata": {
        "id": "8ywvXtWNF9ry"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWQiJhWM8JP8"
      },
      "source": [
        "### Answer (f)\n",
        "\n",
        "Train an ANN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQiRPgG68JP8"
      },
      "outputs": [],
      "source": [
        "# Insert code for part (f)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#create model ANN model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(20, input_shape=(X_train.shape[1],), activation='relu'),\n",
        "    keras.layers.Dense(15, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100)\n",
        "\n",
        "model.evaluate(X_test, y_test)\n",
        "\n",
        "# save the model\n",
        "model.save('churn_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h38v8axb8JP8"
      },
      "source": [
        "Describe what happened and elaborate on model performance. \n",
        "Compare (e) model performance with (f)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.0 32-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "8999ca217b2fa3fb8875d9227edfd6c20266cf632b7380cda2ffe3df3689d7c1"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}